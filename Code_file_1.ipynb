{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f57dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    " \n",
    "DATA_DIR = './data/'\n",
    "PREDICTION_DIR = './prediction/'\n",
    "LATENT_DIM = 128  # Dimesion of the compressed representation\n",
    "EPOCHS_AE = 50      # Epochs for Autoencoder training\n",
    "EPOCHS_PRED = 100   # Epochs for Predictor training\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1.5e-4\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f8372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Original double-gene pairs: 3040\n",
      "Filtered double-gene pairs with complete data: 3000\n",
      "Found 4040 single-gene profiles.\n",
      "Using 3000 double-gene profiles to train the predictor.\n"
     ]
    }
   ],
   "source": [
    "## 2. Data Loading and Preprocessing (Corrected)\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_set.csv'), index_col=0)\n",
    "\n",
    "# Transpose the dataframe so experiments are rows and genes are columns\n",
    "train_df = train_df.T\n",
    "\n",
    "# Separate single and double perturbations\n",
    "single_pert_mask = train_df.index.str.contains('\\+ctrl')\n",
    "double_pert_mask = ~single_pert_mask\n",
    "\n",
    "single_gene_df = train_df[single_pert_mask]\n",
    "double_gene_df = train_df[double_pert_mask]\n",
    "\n",
    "# Create a clean dictionary for single-gene profiles\n",
    "single_gene_df.index = single_gene_df.index.str.replace('\\+ctrl', '', regex=True)\n",
    "single_gene_profiles = single_gene_df.T.to_dict('list')\n",
    "\n",
    "# Convert all data to numpy arrays for easier handling\n",
    "gene_ids = train_df.columns.tolist()\n",
    "X_single_all = single_gene_df.values.astype(np.float32)\n",
    "\n",
    "# --- START: NEW FILTERING LOGIC ---\n",
    "print(f\"Original double-gene pairs: {len(double_gene_df)}\")\n",
    "# Check which double-perturbation pairs have both corresponding single profiles\n",
    "pert_pairs_train = double_gene_df.index.str.split('+', expand=True)\n",
    "gene_A_keys = pert_pairs_train.get_level_values(0)\n",
    "gene_B_keys = pert_pairs_train.get_level_values(1)\n",
    "\n",
    "valid_mask = [(gA in single_gene_profiles and gB in single_gene_profiles) for gA, gB in zip(gene_A_keys, gene_B_keys)]\n",
    "\n",
    "# Apply the mask to keep only the valid pairs\n",
    "double_gene_df = double_gene_df[valid_mask]\n",
    "print(f\"Filtered double-gene pairs with complete data: {len(double_gene_df)}\")\n",
    "# --- END: NEW FILTERING LOGIC ---\n",
    "\n",
    "\n",
    "# Prepare the training data for the Predictor model using the filtered dataframe\n",
    "pert_pairs_train = double_gene_df.index.str.split('+', expand=True)\n",
    "gene_A_train = pert_pairs_train.get_level_values(0)\n",
    "gene_B_train = pert_pairs_train.get_level_values(1)\n",
    "\n",
    "# This will now succeed because we've removed the problematic pairs\n",
    "X_A_train = np.array([single_gene_profiles[g] for g in gene_A_train]).astype(np.float32)\n",
    "X_B_train = np.array([single_gene_profiles[g] for g in gene_B_train]).astype(np.float32)\n",
    "\n",
    "# The target variable is the double-gene profile\n",
    "y_predictor_train = double_gene_df.values.astype(np.float32)\n",
    "\n",
    "print(f\"Found {len(single_gene_df)} single-gene profiles.\")\n",
    "print(f\"Using {len(double_gene_df)} double-gene profiles to train the predictor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f642e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder...\n",
      "AE Epoch [1/50], Loss: 0.675393\n",
      "AE Epoch [2/50], Loss: 0.153221\n",
      "AE Epoch [3/50], Loss: 0.151647\n",
      "AE Epoch [4/50], Loss: 0.147718\n",
      "AE Epoch [5/50], Loss: 0.142288\n",
      "AE Epoch [6/50], Loss: 0.140653\n",
      "AE Epoch [7/50], Loss: 0.139299\n",
      "AE Epoch [8/50], Loss: 0.136445\n",
      "AE Epoch [9/50], Loss: 0.133892\n",
      "AE Epoch [10/50], Loss: 0.132756\n",
      "AE Epoch [11/50], Loss: 0.131555\n",
      "AE Epoch [12/50], Loss: 0.130543\n",
      "AE Epoch [13/50], Loss: 0.129759\n",
      "AE Epoch [14/50], Loss: 0.128376\n",
      "AE Epoch [15/50], Loss: 0.127328\n",
      "AE Epoch [16/50], Loss: 0.126570\n",
      "AE Epoch [17/50], Loss: 0.126566\n",
      "AE Epoch [18/50], Loss: 0.125336\n",
      "AE Epoch [19/50], Loss: 0.124883\n",
      "AE Epoch [20/50], Loss: 0.124714\n",
      "AE Epoch [21/50], Loss: 0.123711\n",
      "AE Epoch [22/50], Loss: 0.122525\n",
      "AE Epoch [23/50], Loss: 0.122275\n",
      "AE Epoch [24/50], Loss: 0.121351\n",
      "AE Epoch [25/50], Loss: 0.120634\n",
      "AE Epoch [26/50], Loss: 0.120473\n",
      "AE Epoch [27/50], Loss: 0.119977\n",
      "AE Epoch [28/50], Loss: 0.118608\n",
      "AE Epoch [29/50], Loss: 0.117781\n",
      "AE Epoch [30/50], Loss: 0.117315\n",
      "AE Epoch [31/50], Loss: 0.116582\n",
      "AE Epoch [32/50], Loss: 0.115925\n",
      "AE Epoch [33/50], Loss: 0.115533\n",
      "AE Epoch [34/50], Loss: 0.114561\n",
      "AE Epoch [35/50], Loss: 0.114262\n",
      "AE Epoch [36/50], Loss: 0.113393\n",
      "AE Epoch [37/50], Loss: 0.112526\n",
      "AE Epoch [38/50], Loss: 0.111835\n",
      "AE Epoch [39/50], Loss: 0.111895\n",
      "AE Epoch [40/50], Loss: 0.110818\n",
      "AE Epoch [41/50], Loss: 0.110294\n",
      "AE Epoch [42/50], Loss: 0.109494\n",
      "AE Epoch [43/50], Loss: 0.108382\n",
      "AE Epoch [44/50], Loss: 0.107863\n",
      "AE Epoch [45/50], Loss: 0.107038\n",
      "AE Epoch [46/50], Loss: 0.106429\n",
      "AE Epoch [47/50], Loss: 0.106104\n",
      "AE Epoch [48/50], Loss: 0.105363\n",
      "AE Epoch [49/50], Loss: 0.104563\n",
      "AE Epoch [50/50], Loss: 0.103992\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1 Define the Autoencoder Model ---\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# --- 3.2 Train the Autoencoder ---\n",
    "print(\"\\nTraining Autoencoder...\")\n",
    "input_dim = X_single_all.shape[1]\n",
    "autoencoder = Autoencoder(input_dim, LATENT_DIM).to(device)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create DataLoader\n",
    "ae_dataset = TensorDataset(torch.from_numpy(X_single_all))\n",
    "ae_loader = DataLoader(ae_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(EPOCHS_AE):\n",
    "    total_loss = 0\n",
    "    for data in ae_loader:\n",
    "        inputs = data[0].to(device)\n",
    "        optimizer_ae.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion_ae(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'AE Epoch [{epoch+1}/{EPOCHS_AE}], Loss: {total_loss/len(ae_loader):.6f}')\n",
    "\n",
    "# Separate encoder and decoder for later use\n",
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a311566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Predictor Model...\n",
      "Predictor Epoch [1/100], Loss: 2.073958\n",
      "Predictor Epoch [2/100], Loss: 0.514933\n",
      "Predictor Epoch [3/100], Loss: 0.406230\n",
      "Predictor Epoch [4/100], Loss: 0.389918\n",
      "Predictor Epoch [5/100], Loss: 0.376492\n",
      "Predictor Epoch [6/100], Loss: 0.365632\n",
      "Predictor Epoch [7/100], Loss: 0.356757\n",
      "Predictor Epoch [8/100], Loss: 0.349047\n",
      "Predictor Epoch [9/100], Loss: 0.342167\n",
      "Predictor Epoch [10/100], Loss: 0.336875\n",
      "Predictor Epoch [11/100], Loss: 0.334087\n",
      "Predictor Epoch [12/100], Loss: 0.330117\n",
      "Predictor Epoch [13/100], Loss: 0.327400\n",
      "Predictor Epoch [14/100], Loss: 0.324605\n",
      "Predictor Epoch [15/100], Loss: 0.324005\n",
      "Predictor Epoch [16/100], Loss: 0.320371\n",
      "Predictor Epoch [17/100], Loss: 0.316862\n",
      "Predictor Epoch [18/100], Loss: 0.316506\n",
      "Predictor Epoch [19/100], Loss: 0.315259\n",
      "Predictor Epoch [20/100], Loss: 0.312726\n",
      "Predictor Epoch [21/100], Loss: 0.312280\n",
      "Predictor Epoch [22/100], Loss: 0.310915\n",
      "Predictor Epoch [23/100], Loss: 0.309377\n",
      "Predictor Epoch [24/100], Loss: 0.307267\n",
      "Predictor Epoch [25/100], Loss: 0.306453\n",
      "Predictor Epoch [26/100], Loss: 0.303634\n",
      "Predictor Epoch [27/100], Loss: 0.304111\n",
      "Predictor Epoch [28/100], Loss: 0.302480\n",
      "Predictor Epoch [29/100], Loss: 0.301104\n",
      "Predictor Epoch [30/100], Loss: 0.299213\n",
      "Predictor Epoch [31/100], Loss: 0.298927\n",
      "Predictor Epoch [32/100], Loss: 0.298394\n",
      "Predictor Epoch [33/100], Loss: 0.297091\n",
      "Predictor Epoch [34/100], Loss: 0.297007\n",
      "Predictor Epoch [35/100], Loss: 0.295456\n",
      "Predictor Epoch [36/100], Loss: 0.295096\n",
      "Predictor Epoch [37/100], Loss: 0.293109\n",
      "Predictor Epoch [38/100], Loss: 0.292121\n",
      "Predictor Epoch [39/100], Loss: 0.291563\n",
      "Predictor Epoch [40/100], Loss: 0.291372\n",
      "Predictor Epoch [41/100], Loss: 0.290161\n",
      "Predictor Epoch [42/100], Loss: 0.289738\n",
      "Predictor Epoch [43/100], Loss: 0.290508\n",
      "Predictor Epoch [44/100], Loss: 0.288687\n",
      "Predictor Epoch [45/100], Loss: 0.287476\n",
      "Predictor Epoch [46/100], Loss: 0.286113\n",
      "Predictor Epoch [47/100], Loss: 0.285678\n",
      "Predictor Epoch [48/100], Loss: 0.286331\n",
      "Predictor Epoch [49/100], Loss: 0.286229\n",
      "Predictor Epoch [50/100], Loss: 0.286253\n",
      "Predictor Epoch [51/100], Loss: 0.284357\n",
      "Predictor Epoch [52/100], Loss: 0.283683\n",
      "Predictor Epoch [53/100], Loss: 0.282867\n",
      "Predictor Epoch [54/100], Loss: 0.283088\n",
      "Predictor Epoch [55/100], Loss: 0.280986\n",
      "Predictor Epoch [56/100], Loss: 0.281577\n",
      "Predictor Epoch [57/100], Loss: 0.280769\n",
      "Predictor Epoch [58/100], Loss: 0.280163\n",
      "Predictor Epoch [59/100], Loss: 0.280125\n",
      "Predictor Epoch [60/100], Loss: 0.279972\n",
      "Predictor Epoch [61/100], Loss: 0.279505\n",
      "Predictor Epoch [62/100], Loss: 0.277447\n",
      "Predictor Epoch [63/100], Loss: 0.277357\n",
      "Predictor Epoch [64/100], Loss: 0.277915\n",
      "Predictor Epoch [65/100], Loss: 0.277437\n",
      "Predictor Epoch [66/100], Loss: 0.277174\n",
      "Predictor Epoch [67/100], Loss: 0.276107\n",
      "Predictor Epoch [68/100], Loss: 0.274974\n",
      "Predictor Epoch [69/100], Loss: 0.276125\n",
      "Predictor Epoch [70/100], Loss: 0.274596\n",
      "Predictor Epoch [71/100], Loss: 0.275157\n",
      "Predictor Epoch [72/100], Loss: 0.273709\n",
      "Predictor Epoch [73/100], Loss: 0.273434\n",
      "Predictor Epoch [74/100], Loss: 0.273325\n",
      "Predictor Epoch [75/100], Loss: 0.272137\n",
      "Predictor Epoch [76/100], Loss: 0.273058\n",
      "Predictor Epoch [77/100], Loss: 0.271175\n",
      "Predictor Epoch [78/100], Loss: 0.271314\n",
      "Predictor Epoch [79/100], Loss: 0.271015\n",
      "Predictor Epoch [80/100], Loss: 0.272335\n",
      "Predictor Epoch [81/100], Loss: 0.269128\n",
      "Predictor Epoch [82/100], Loss: 0.271532\n",
      "Predictor Epoch [83/100], Loss: 0.269938\n",
      "Predictor Epoch [84/100], Loss: 0.269922\n",
      "Predictor Epoch [85/100], Loss: 0.269022\n",
      "Predictor Epoch [86/100], Loss: 0.269295\n",
      "Predictor Epoch [87/100], Loss: 0.268414\n",
      "Predictor Epoch [88/100], Loss: 0.268232\n",
      "Predictor Epoch [89/100], Loss: 0.267389\n",
      "Predictor Epoch [90/100], Loss: 0.267327\n",
      "Predictor Epoch [91/100], Loss: 0.267909\n",
      "Predictor Epoch [92/100], Loss: 0.266035\n",
      "Predictor Epoch [93/100], Loss: 0.267515\n",
      "Predictor Epoch [94/100], Loss: 0.266574\n",
      "Predictor Epoch [95/100], Loss: 0.265311\n",
      "Predictor Epoch [96/100], Loss: 0.265015\n",
      "Predictor Epoch [97/100], Loss: 0.264003\n",
      "Predictor Epoch [98/100], Loss: 0.264178\n",
      "Predictor Epoch [99/100], Loss: 0.265544\n",
      "Predictor Epoch [100/100], Loss: 0.264692\n"
     ]
    }
   ],
   "source": [
    "# --- 4.1 Define the Predictor Model ---\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 256), # Takes concatenated latent vectors\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim) # Outputs a predicted latent vector\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- 4.2 Transform data to latent space and train ---\n",
    "print(\"\\nTraining Predictor Model...\")\n",
    "\n",
    "# Transform all data to latent space using the trained encoder\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    X_A_latent = encoder(torch.from_numpy(X_A_train).to(device)).cpu().numpy()\n",
    "    X_B_latent = encoder(torch.from_numpy(X_B_train).to(device)).cpu().numpy()\n",
    "    y_predictor_latent = encoder(torch.from_numpy(y_predictor_train).to(device)).cpu().numpy()\n",
    "\n",
    "# Concatenate the latent vectors of gene A and B\n",
    "X_predictor_latent = np.concatenate([X_A_latent, X_B_latent], axis=1)\n",
    "\n",
    "# Create DataLoader for the predictor\n",
    "pred_dataset = TensorDataset(torch.from_numpy(X_predictor_latent), torch.from_numpy(y_predictor_latent))\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize and train the predictor\n",
    "predictor = Predictor(LATENT_DIM).to(device)\n",
    "criterion_pred = nn.MSELoss()\n",
    "optimizer_pred = optim.Adam(predictor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS_PRED):\n",
    "    total_loss = 0\n",
    "    for data in pred_loader:\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_pred.zero_grad()\n",
    "        outputs = predictor(inputs)\n",
    "        loss = criterion_pred(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_pred.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Predictor Epoch [{epoch+1}/{EPOCHS_PRED}], Loss: {total_loss/len(pred_loader):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fff4c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for the test set...\n",
      "Formatting submission file...\n",
      "\n",
      "✅ Successfully created submission file at: ./prediction/prediction.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating predictions for the test set...\")\n",
    "\n",
    "# Load test set gene pairs\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_set.csv'), header=None)\n",
    "test_df.columns = ['perturbation']\n",
    "pert_pairs_test = test_df['perturbation'].str.split('+', expand=True)\n",
    "gene_A_test = pert_pairs_test[0]\n",
    "gene_B_test = pert_pairs_test[1]\n",
    "\n",
    "# Get single-gene profiles for test pairs\n",
    "X_A_test = np.array([single_gene_profiles[g] for g in gene_A_test]).astype(np.float32)\n",
    "X_B_test = np.array([single_gene_profiles[g] for g in gene_B_test]).astype(np.float32)\n",
    "\n",
    "# Full prediction pipeline\n",
    "encoder.eval()\n",
    "predictor.eval()\n",
    "decoder.eval()\n",
    "final_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 1. Encode single profiles\n",
    "    latent_A = encoder(torch.from_numpy(X_A_test).to(device))\n",
    "    latent_B = encoder(torch.from_numpy(X_B_test).to(device))\n",
    "\n",
    "    # 2. Predict combined latent profile\n",
    "    latent_AB_input = torch.cat([latent_A, latent_B], dim=1)\n",
    "    predicted_latent_AB = predictor(latent_AB_input)\n",
    "\n",
    "    # 3. Decode back to full gene expression space\n",
    "    predicted_expression_AB = decoder(predicted_latent_AB).cpu().numpy()\n",
    "\n",
    "# --- Format for submission ---\n",
    "print(\"Formatting submission file...\")\n",
    "submission_list = []\n",
    "for i, pert_name in enumerate(test_df['perturbation']):\n",
    "    for j, gene_id in enumerate(gene_ids):\n",
    "        submission_list.append({\n",
    "            'gene': gene_id,\n",
    "            'perturbation': pert_name,\n",
    "            'expression': predicted_expression_AB[i, j]\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_list)\n",
    "\n",
    "# Save to CSV\n",
    "if not os.path.exists(PREDICTION_DIR):\n",
    "    os.makedirs(PREDICTION_DIR)\n",
    "submission_path = os.path.join(PREDICTION_DIR, 'prediction.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Successfully created submission file at: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cadde886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.17627"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
